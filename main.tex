\documentclass[
	%a4paper, % Use A4 paper size
	letterpaper, % Use US letter paper size
]{jdf}

\addbibresource{references.bib}

\author{Mohamed Fayed}
\email{mohamed.fayed.425@gmail.com}
\title{Benchmarking Gemma Models}

\begin{document}
%Benchmark Gemma Models: Develop a comprehensive benchmark suite to test Gemma models on a range of tasks and datasets (academic benchmarks like MMLU, GSM8K, etc., and custom datasets).
%
%Create scripts for automation. Automate the benchmarking process for various tasks.
%Compare performance of various Gemma model families. Run the benchmarks on different Gemma model sizes and variants.
%Compare performance against other open models. Include other popular open models (e.g., Llama 2, Mistral) in the benchmark for comparison.
%Create informative summaries of benchmark results. Generate reports and visualizations summarizing the results. This should include tables, charts, and potentially a leaderboard.
%Regular Updates: Design the benchmark to be easily updated with new Gemma models and datasets.
%Reproducibility: Provide clear instructions and scripts to allow others to reproduce the benchmark results.
%Complexity: Medium to High. Requires understanding of benchmarking methodologies, machine learning evaluation, and scripting.
%
%Expected Size: 175-350 hours.
%
%Skills: Python, machine learning evaluation, scripting, data analysis.

\begin{abstract}
Gemma models has shown impressive capabilities on many benchmarks.
However, the authors did not test other open source models against Gemma because their setting does not guarantee for fairness among all of them. %check report page 5.
    Moreover, there are more specialized benchmarks on which the performance of Gemma is yet to be determined.
    In our proposal for GSoC 2025, we suggest coding a tool for evaluating a set of Text only and Multimodal Large Language Models.
     \end{abstract}

\section{Introduction}
% landscape
% problems
% contributions
During the course of this project, I aim at developping a framework to evaluate Multimodal Large Language Models of the following features:
     \begin{itemize}
              \item Support for diverse set of benchmark datasets,
              \item Build an interface for Huggingface Models to support many models including Gemma, LLaMA, Mistral and other models released in the future.
              \item Easy interface to run on benchmark datasets, and
              \item A leaderboard including all results for models tested with our framework.
                   \end{itemize}

\section{Benchmarks}\label{sect:benchmarks}
There has been a lot of benchmarks introduced to test MLLMs.
So, I select a sample of them to be representative to many categories from development perspective.
For other benchmark testsets, they are expected to be added at a later stage to test the modularity of the framework.
\subsection{Commonly Used Benchmarks}\label{ssect:common}
This is a list of commonly used benchmarks in reporting LLMs/MLLMs performance:
\begin{enumerate}
    \item MMLU-PRO~\cite{wang2024mmlu} and/or MMLU-ProX~\cite{xuan2025mmlu}
    \item TruthfulQA\cite{lin2021truthfulqa}
    \item HellaSwag~\cite{zellers2019hellaswag} and/or HellaSwag-Pro~\cite{li2025hellaswag}
    \item Big-Bench Lite~\cite{srivastava2023beyond}
    \item IFEval~\cite{zhou2023instruction} and/or IFEval-Extended~\cite{kovalevskyi2024ifeval}
              \end{enumerate}
              \subsection{Coding Tasks}
              \begin{itemize}
    \item CodeXGLUE~\cite{lu2021codexglue}
                            \end{itemize}
                            \subsection{Chart Related Tasks}
\subsubsection{Chart-to-Table}
\begin{itemize}
    \item Testsets: ChartQA~\cite{masry2022chartqa}, PlotQA~\cite{methani2020plotqa}, ICPR22
         \item Metrics: Relative Number Set Similarity and Relative Mapping Similarity~\cite{liu2022deplot}
              \end{itemize}
\subsubsection{Chart Question Answering}
\begin{itemize}
    \item Testsets: ChartQA~\cite{masry2022chartqa}, PlotQA~\cite{methani2020plotqa}, ICPR22
         \item Metrics: Accuracy, Precision, Recall and F1
              \end{itemize}
\subsubsection{Chart Summarization}
\begin{itemize}
    \item Testsets: Chart-to-Text~\cite{kantharaj2022chart} and ChartSumm~\cite{rahman2023chartsumm}
    \item Metrics: BLEU~\cite{post2018call}, CIDEr~\cite{vedantam2015cider}, ROUGE~\cite{lin2004rouge} and BLEURT~\cite{sellam2020bleurt}.
              \end{itemize}
\subsection{LLMs as Agents}
\begin{itemize}
    \item Software Engineering Agents: SWE-Bench~\cite{yang2024swe}
    \item Machine Learning Researchers: MLAgentBench~\cite{huang2023mlagentbench}, MLGym~\cite{nathani2025mlgym}
              \end{itemize}
\subsection{Task Selection}\label{app:selected-tasks}
The following is the list of selected benchmark to implement: TBD

We have two main componenets: MLLM evaluation and Leaderboard.

To illustrate the role for each componenet, assume the following scenario:
\begin{enumerate}
         \item The researchers want to evaluate their model during Instruction Fine-tuning process or a model that was generated due to a previous tuning step.
             So, they use MLLM Evaluation tool via python function calling or command line interface, respectively.
         \item If they provide a link to the Leaderboard, MLLM Evaluation tool communicates with the Leaderboard via http/https to send the results.
         \item The researchers could track the tables in the Leaderboard and see their cells getting filled with evaluation results.
              \end{enumerate}

\section{Technical Implementation Plan}
In the following subsections, I explain in details the steps to make the evaluation framework.
In table~\ref{tab:timeline}, you can find the list of tasks with their expected deadlines.
\input{timetable}
\subsection{MLLM Evaluation Core}
First, I will write down the main skeleton of the tool.
This includes: 
\begin{enumerate}
         \item Implement the main function for Gemma 3 4B on single dataset,
         \item taking arguments via both command line and function calling,
         \item evaluating the model on one benchmark, and
         \item reporting the results in form of printed text (output of CLI).
              \end{enumerate}
\subsection{Add support for some models and datasets}
Update the code to work with other models like LLaMA and Mistral, and the other datasets~\ref{ssect:selected-tasks}.
%               \subsection{Multidataset and Multimodels run}
%               This step includes updating the main function to support running evaluation of many models on many datasets.
%               Additionally, it includes running large models on more than one compute device (GPU/TPU) and running the whole set of models
%\subsection{Interfaces}
\subsection{Leaderboard}
I plan to make the leaderboard a web application that shows the results of the evaluation for all models.
Other evaluation scripts are expected to contact this app with the final scores via http/https requests.
For frameworks, I plan to use FastAPI, Angular and Nginx.
% Another solution is to upload to tensorboard, but I am not sure whether it will display properly.

\section{Deliverables}
By the end of this project, I expect to deliver:
\begin{enumerate}
         \item Evaluation Git Repo: repository containing the code for MLLM evaluation,
         \item Leaderboard Repo: repository containing the code for Leaderboard web app, and
         \item System Paper: explaining the overall system and its design choices.
              \end{enumerate}

\section{Anticipated Impact}\label{sect:impact}
By the end of this project, we end up with an MLLM evaluation tools that make it easier to evaluate MLLMs, compare among them and understand their capabilities in fine-grained details across various general and domain specific tasks.

\section{References}
\printbibliography[heading=none]

\end{document}
