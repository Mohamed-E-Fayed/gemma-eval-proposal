{\rtf1\ansi\ansicpg1252\cocoartf2820
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Answering \'93Tell us about why you're interested in this project, and relevant experiences and skills\'94 for each project:\
\
Project 1\
\
I am excited about this project because it combines my passion for large language models with my interest in learning JAX and writing down high quality tutorials. I have experience fine-tuning LLMs using PyTorch and Hugging Face accelerate library, leveraging optimization techniques such as DeepSpeed\'92s ZeRO optimizations to make vanilla fine-tuning large models possible in limited GPU setup. I enjoy making good tutorials and documentation at my employer\'92s internal wiki to teach and strengthen my understanding for topics in hand. You can find a couple of articles on my website at {\field{\*\fldinst{HYPERLINK "https://mohamed-e-fayed.github.io/"}}{\fldrslt https://mohamed-e-fayed.github.io}}. This project aligns well with my goal of learning JAX while contributing clear, well-documented educational materials to help other developers better understand LLM training and inference optimizations.\
\
Project 2:\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 I am excited about this project because it combines my passion for large language models (LLMs) with my interest in learning JAX and writing down high quality tutorials. I have experience coding custom language models and LLM fine-tuning. I use PyTorch, and Hugging Face transformers and accelerate frameworks. I have interest to learn more about most recent LLMs, especially Multimodal LLMs. I enjoy making high quality tutorials and documentation at my employer\'92s internal wiki to teach and strengthen my understanding for topics in hand. You can find a couple of articles on my website at {\field{\*\fldinst{HYPERLINK "https://mohamed-e-fayed.github.io/"}}{\fldrslt https://mohamed-e-fayed.github.io}}. This project aligns well with my goal of learning JAX while contributing clear, well-documented educational materials to help other developers better understand LLM internals.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Project 3:\
\
I am highly interested in this project because it aligns with my experience in fine-tuning large language models and my desire to learn about LLM tools integration. I have fine-tuned LLMs using various frameworks including PyTorch, transformers and LlamaFactory. Additionally, I have experience working with transformer architectures and understand the challenges of training models for executable code generation and domain-specific question answering. I want to learn more about integrating LLMs with external tools like python interpreter because it is crucial in training the model on creating executable code. Beyond my technical skills, I am passionate about education\'97I enjoy writing down tutorials and documentation in our internal wiki. This project would allow me to learn more about integrating LLMs-tools integration, and to contribute to the JAX ecosystem while continuing to create accessible learning resources for the researchers and developers community.}